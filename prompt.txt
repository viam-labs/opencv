Read over ./src/models and ./src/scripts as well as 
the readme in this directory to get an idea of what 
the module currently does. My goal is to implement a pose_test model
in ./src/models that measures (slightly somewhat similarly)
to touch_test.py, the error between ground truth and our configured
camera hand-eye calibration. It should invoke the following algorithm with a DoCommand.
The model should be a generic service.

Let's start with some definitions. A is the pose of the hand (arm's final wrist joint)
in the world frame, B is the pose of the camera in the world state. And X
is the hand eye transformation.

The way the pose test will work is that we will collect n poses of the arm with the
camera mounted on the hand. The first pose (pose_0) will be our "anchor" of sorts. As in,
all following poses will be relative to pose_0. We'll calculate delta B by matrix multiplying
the inverse of B_i to B_0. We'll calculate delta estimated A by multiplying X * the aforementioned delta B
* the inverse of X. This will give us the delta A estimated because it will be where we "think" the hand is
based on our X. We then get the ground truth delta A from the arm itself (via some get pose call). 

We then calculate the translational error by taking the abs val of the difference of the t vectors from the
two delta matrices, and calculate the rotational error by R_A_predicted^T * R_A_actual, then taking calculating
the Rodriguez and measuring how large the theta is.

Outside of the algorithm itself, let's start thinking about the resource itself.
The config should have a list of poses (ox, oy, oz, x, y, z). Assume index 0 of that list is pose_0. It should have a dependency
on a pose tracker which should also be an explicit config attribute. It should also depend on the
motion service to get poses, move to different poses, and perform frame system transforms. 

We should expose an additional do command to get the current pose of the arm in the world frame, so that
users have an easy way to make the list of poses.

That is the high level. Please fill in the gaps in this algorithm and the model implementation.
Organize this prompt into a thorough step-by-step algorithm implementation plan AS WELL as a model API/user
experience plan e.g. what the config should look like, what APIs (docommands) are exposed and their args and response values. 

Conversation context:

âº Pose Test Implementation Summary

  Context

  We implemented a new pose-test model for the opencv Viam module based on the specifications in prompt.txt. The goal was to create a service that tests
  hand-eye calibration accuracy by measuring pose estimation errors across multiple arm positions.

  What Was Implemented

  1. Extended go_utils Binary (go_utils/main.go)

  Added two new commands to handle full pose transformations using Viam's spatialmath library:

  - compose: Composes two poses (takes 14 floats for two full poses, returns 7 floats)
    - Uses: spatialmath.Compose(pose1, pose2)
  - inverse_pose: Inverts a pose (takes 7 floats, returns 7 floats)
    - Uses: spatialmath.PoseInverse(pose)

  Key Fix: Fixed typo in compose function where X: y2 was incorrectly written instead of X: x2 (line 81)

  All existing commands (ov2mat, mat2ov) remain backward compatible.

  2. Python Utilities (src/utils/utils.py)

  Added Python wrapper functions:
  - call_go_compose(pose1: Pose, pose2: Pose) -> Pose
  - call_go_inverse_pose(pose: Pose) -> Pose

  3. New Model: viam:opencv:pose-test (src/models/pose_test.py)

  Architecture Decision: Algorithm uses 4x4 homogeneous transformation matrices internally for all calculations, with conversions only at boundaries
  (input/output).

  Key Components:

  Configuration:
  {
    "arm_name": str,        # Required
    "camera_name": str,     # Required (added as proper dependency)
    "pose_tracker": str,    # Required
    "motion_service": str,  # Required
    "world_frame": str,     # Optional (default: "world")
    "poses": [              # Required (min 2 poses)
      {"x": float, "y": float, "z": float, "o_x": float, "o_y": float, "o_z": float, "theta": float}
    ]
  }

  Helper Methods:
  - _pose_to_matrix(pose: Pose) -> np.ndarray: Converts Viam Pose to 4x4 matrix using call_go_ov2mat
  - _matrix_to_pose(T: np.ndarray) -> Pose: Converts 4x4 matrix to Viam Pose using call_go_mat2ov

  Error Calculations (Matrix-Based):
  - _calculate_translational_error(T_estimated, T_actual):
    - Extracts translation vectors: t = T[0:3, 3]
    - Returns delta_x, delta_y, delta_z, and magnitude
  - _calculate_rotational_error(T_estimated, T_actual):
    - Extracts 3x3 rotation matrices: R = T[0:3, 0:3]
    - Computes relative rotation: R_error = R_estimated^T @ R_actual
    - Extracts angle using trace: theta = arccos((trace(R) - 1) / 2)
    - Returns angle in degrees

  Algorithm (_run_pose_test_algorithm):
  1. Move to pose_0 (anchor), get A_0 and B_0, convert to matrices
  2. Compute hand-eye transform: T_X = T_A_0 @ inv(T_B_0)
  3. For each test pose i:
    - Get A_i and B_i, convert to matrices
    - Calculate T_delta_B = inv(T_B_i) @ T_B_0
    - Calculate T_delta_A_estimated = T_X @ T_delta_B @ inv(T_X)
    - Calculate T_delta_A_actual = inv(T_A_i) @ T_A_0
    - Compare estimated vs actual using matrix-based error calculations
  4. Return per-pose errors and aggregate statistics

  DoCommand API:
  - get_current_pose: Returns current arm pose in world frame (helper for collecting poses)
  - validate_setup: Pre-flight check of dependencies and configuration
  - run_pose_test: Executes full algorithm, returns comprehensive results

  4. Registration (src/main.py)

  Added import for PoseTest model to register it with the module.

  5. Documentation (README.md)

  Added comprehensive section following existing model conventions:
  - Configuration template and attributes table
  - Example configuration with camera_name
  - All three DoCommand APIs with request/response examples
  - Algorithm explanation
  - Complete workflow guide
  - Error interpretation guidelines

  Key Design Decisions

  1. Matrix-First Architecture: All pose math uses 4x4 homogeneous transformation matrices internally. Poses are converted to matrices immediately on input and
  only converted back for logging/output.
  2. Proper Camera Dependency: Added camera_name as required config attribute instead of hacky string parsing from pose tracker name.
  3. Mathematically Rigorous Rotation Error: Uses proper rotation matrix trace formula instead of approximations.
  4. Backward Compatibility: All changes to go_utils maintain backward compatibility with existing code.

  Files Modified/Created

  - go_utils/main.go - Added compose and inverse_pose commands
  - src/utils/utils.py - Added Python wrappers
  - src/models/pose_test.py - New model (515 lines)
  - src/main.py - Registered new model
  - README.md - Added comprehensive documentation
